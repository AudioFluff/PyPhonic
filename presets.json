[
    {
        "id": 1,
        "name": "Init (Plain Python)",
        "code": "import pyphonic\ndef process(midi, audio):\n    return midi, audio"
    },
    {
        "id": 2,
        "name": "Init (Python with NumPy)",
        "code": "import numpy as np\nimport pyphonic\ndef process_npy(midi, audio):\n    return midi, audio"
    },
    {
        "id": 3,
        "name": "Init (Python with PyTorch)",
        "code": "import torch\nimport pyphonic\ndef process_torch(midi, audio):\n    return midi, audio"
    },
    {
        "id": 4,
        "name": "Butterworth Filter (High, Low or Bandpass)",
        "code": "import pyphonic\n\nimport numpy as np\nfrom scipy.signal import butter, sosfilt\n\norder = 6\nfs = None\nsos = None\ncutoff = 500\nZl, Zr = None, None\n\ndef process_npy(midi, audio):\n    global Zl, Zr, fs, sos\n    if fs is None:\n        fs = pyphonic.getSampleRate()\n        sos = butter(order, cutoff, fs=fs, btype='lowpass', analog=False, output='sos')\n        Zl = np.zeros((sos.shape[0], 2))\n        Zr = np.zeros((sos.shape[0], 2))\n        return midi, audio\n    left = audio[:audio.shape[0]//2]\n    right = audio[audio.shape[0]//2:]\n    left, Zl = sosfilt(sos, left, zi=Zl)\n    right, Zr = sosfilt(sos, right, zi=Zr)\n    return midi, np.concatenate((left, right))"
    },
    {
        "id": 5,
        "name": "Midi Arpeggiator, minor triad",
        "code": "import pyphonic\nfrom pyphonic import MidiMessage\n\ntimer = 0\nmidibuf = {}\ninitial_delay = 0\nduration = 1\narp_delay = 7\n\ndef add_note(beats_from_now, type, note, velocity, channel):\n    global midibuf, timer\n\n    bpm = pyphonic.getBPM()\n    \n    bps = bpm / 60\n    ticks_per_second = bps * 1000\n\n    blocks_per_second = pyphonic.getSampleRate() / pyphonic.getBlockSize()\n\n    ticks_per_block = ticks_per_second / blocks_per_second\n\n    tick_skip = 1000 * beats_from_now\n    \n    blocks_in_future = tick_skip / ticks_per_block / 4 # assume 4/4 time\n\n    when = int(timer + blocks_in_future)\n\n    midibuf[when] = midibuf.get(when, [])\n\n    new_note = MidiMessage(type, note, velocity, channel)\n    if type == 'note_off':\n        new_note.velocity = 0\n    midibuf[when].append(new_note)\n\ndef process(midi, audio):\n    global timer, midibuf\n    for msg in midi:\n        if msg.type == 'note_on':\n            when = initial_delay\n            add_note(when, 'note_on', msg.note, msg.velocity, msg.channel)\n            when += duration\n            add_note(when, 'note_off', msg.note, msg.velocity, msg.channel)\n            when += arp_delay\n            add_note(when, 'note_on', msg.note + 3, msg.velocity, msg.channel)\n            when += duration\n            add_note(when, 'note_off', msg.note + 3, msg.velocity, msg.channel)\n            when += arp_delay\n            add_note(when, 'note_on', msg.note + 7, msg.velocity, msg.channel)\n            when += duration\n            add_note(when, 'note_off', msg.note + 7, msg.velocity, msg.channel)\n    \n    for k in list(midibuf.keys()):\n        if k < timer:\n            del midibuf[k]\n\n    timer += 1\n\n    return midibuf.get(timer - 1, []), audio"
    },
    {
        "id": 6,
        "name": "Wavetable Synth",
        "code": "from pathlib import Path\n\nimport pyphonic\nfrom pyphonic import MidiMessage\nimport numpy as np\nimport librosa\n\nsample = np.load(pyphonic.getDataDir() / Path(\"glockenspiel.pkl\"), allow_pickle=True)\n\ndef noteToFreq(midi_note):\n    a = 440\n    return (a / 32) * (2 ** ((midi_note - 9) / 12))\n\nprint(\"Building wavetable\")\nvoices = {}\nfor note in range(31, 103): # G1 to G7\n    print(f\"Building wavetable {note}\")\n    if note == 60:\n        ratio = 1 # Center on C3\n    else:\n        freq = noteToFreq(note)\n        ratio = freq / 261.63\n    left = librosa.effects.pitch_shift(sample[0], sr=44100, n_steps=(note - 60))\n    right = librosa.effects.pitch_shift(sample[1], sr=44100, n_steps=(note - 60))\n    joined = np.array([left, right])\n    voices[note] = {\"wave\": joined, \"position\": 0, \"playing\": False, \"velocity\": 0}\n\ndef process_npy(midi, audio):\n\n    for msg in midi:\n        if msg.note not in voices:\n            continue\n        if msg.type == \"note_on\":\n            if voices[msg.note][\"playing\"]:\n                voices[msg.note][\"position\"] = 0\n            else:\n                voices[msg.note][\"playing\"] = True\n            voices[msg.note][\"velocity\"] = msg.velocity\n        elif msg.type == \"note_off\":\n            voices[msg.note][\"position\"] = 0\n            voices[msg.note][\"playing\"] = False\n            # Could add some tail off instead of dead stop\n    \n    new_audio = np.zeros((pyphonic.getNumChannels(), pyphonic.getBlockSize()))\n\n    for voice, data in voices.items():\n        if not data[\"playing\"]:\n            continue\n        start_pos = data[\"position\"] % data[\"wave\"].shape[1]\n        end_pos = (start_pos + pyphonic.getBlockSize())\n        if end_pos >= data[\"wave\"].shape[1]:\n            end_pos = data[\"wave\"].shape[1]\n            new_audio[:, :end_pos - start_pos] += data[\"wave\"][:, start_pos:end_pos]\n            voices[voice][\"position\"] = 0\n            voices[voice][\"playing\"] = False\n        else:\n            new_audio += data[\"wave\"][:, start_pos:end_pos] * (data[\"velocity\"] / 127)\n            voices[voice][\"position\"] += pyphonic.getBlockSize()\n    \n    return midi, new_audio"
    },
    {
        "id": 7,
        "name": "Time Stretching Sampler",
        "code": "from pathlib import Path\n\nimport pyphonic\nfrom pyphonic import MidiMessage\nimport numpy as np\nimport librosa\n\nsample = np.load(pyphonic.getDataDir() / Path(\"glockenspiel.pkl\"), allow_pickle=True)\n\ndef noteToFreq(midi_note):\n    a = 440\n    return (a / 32) * (2 ** ((midi_note - 9) / 12))\n\nprint(\"Building wavetable\")\nvoices = {}\nfor note in range(31, 103): # G1 to G7\n    print(f\"Building wavetable {note}\")\n    if note == 60:\n        ratio = 1 # Center on C3\n    else:\n        freq = noteToFreq(note)\n        ratio = freq / 261.63\n    left = librosa.effects.time_stretch(sample[0], rate=ratio)\n    right = librosa.effects.time_stretch(sample[1], rate=ratio)\n    joined = np.array([left, right])\n    voices[note] = {\"wave\": joined, \"position\": 0, \"playing\": False, \"velocity\": 0}\n\ndef process_npy(midi, audio):\n\n    for msg in midi:\n        if msg.note not in voices:\n            continue\n        if msg.type == \"note_on\":\n            if voices[msg.note][\"playing\"]:\n                voices[msg.note][\"position\"] = 0\n            else:\n                voices[msg.note][\"playing\"] = True\n            voices[msg.note][\"velocity\"] = msg.velocity\n        elif msg.type == \"note_off\":\n            voices[msg.note][\"position\"] = 0\n            voices[msg.note][\"playing\"] = False\n    \n    new_audio = np.zeros((pyphonic.getNumChannels(), pyphonic.getBlockSize()))\n\n    for voice, data in voices.items():\n        if not data[\"playing\"]:\n            continue\n        start_pos = data[\"position\"] % data[\"wave\"].shape[1]\n        end_pos = (start_pos + pyphonic.getBlockSize())\n        if end_pos >= data[\"wave\"].shape[1]:\n            end_pos = data[\"wave\"].shape[1]\n            new_audio[:, :end_pos - start_pos] += data[\"wave\"][:, start_pos:end_pos]\n            voices[voice][\"position\"] = 0\n        else:\n            new_audio += data[\"wave\"][:, start_pos:end_pos] * (data[\"velocity\"] / 127)\n            voices[voice][\"position\"] += pyphonic.getBlockSize()\n    \n    return midi, new_audio"
    },
    {
        "id": 8,
        "name": "Noise (PyTorch)",
        "code": "import torch\n\nimport pyphonic\n\nNOISE_VOLUME = 0.1\n# 0.01 is a vinyl crackle, 0.1 is a light rain, 0.5 is a heavy rain, 1 is a thunderstorm\nNOISE_DENSITY = NOISE_VOLUME * 0.5\n\ndef process_torch(midi, audio):\n    noise = torch.randn_like(audio) * NOISE_VOLUME\n    noise = noise * (torch.rand_like(noise).abs() < NOISE_DENSITY)\n    return midi, audio + noise"
    },
    {
        "id": 9,
        "name": "Saturator (PyTorch)",
        "code": "# Use pitch wheel to control saturation level\n\nimport torch\n\nimport pyphonic\n\n# This adds oomph to EDM drums\n# DEW_POINT = 0.9\n# SHIFT = 0\n# FOLDBACK_POINT = 0.8\n# COMP_GAIN = 1.1\n\n# This is a Brit guitar amp\nDEW_POINT = 0.1\nSHIFT = 0.005\nFOLDBACK_POINT = 0.8\nCOMP_GAIN = 3.0\n\nmodified_dew_point = DEW_POINT\n\ndef process_torch(midi, audio):\n    global modified_dew_point\n    for cc in midi:\n        if cc.type == \"pitch_wheel_change\":\n            modified_dew_point = DEW_POINT * ((cc.velocity + 127) / 127)\n            print(modified_dew_point)\n\n    audio[audio < DEW_POINT * audio.min()] = DEW_POINT * audio.min() + SHIFT\n    audio[audio > DEW_POINT * audio.max()] = DEW_POINT * audio.max() + SHIFT\n    audio[audio > FOLDBACK_POINT] = FOLDBACK_POINT - audio[audio > FOLDBACK_POINT]\n    audio[audio < -FOLDBACK_POINT] = -FOLDBACK_POINT + -audio[audio < -FOLDBACK_POINT]\n    return midi, audio * COMP_GAIN"
    },
    {
        "id": 10,
        "name": "Ramping FFT-based filter",
        "code": "# fft\n# Ramped FFT-based filtering over the FFT bins specified as TAMPER_START and TAMPER_END\n\nimport numpy as np\nfrom scipy import signal\n\nimport pyphonic\n\nBUF_SIZE = 8820\n\nstored_buffer_left = np.zeros((BUF_SIZE, ), dtype=np.float32)\nstored_buffer_right = np.zeros((BUF_SIZE, ), dtype=np.float32)\noutput_buffer_left = np.zeros((BUF_SIZE, ), dtype=np.float32)\noutput_buffer_right = np.zeros((BUF_SIZE, ), dtype=np.float32)\n\nread_stored, write_stored = 0, 0\nread_output, write_output = 0, 0\nstarted = False\n\nTAMPER_START, TAMPER_END = 10, 40\n\ndef wrapped_write(data, buf, ptr):\n    data_len = data.shape[0]\n    if ptr + data_len > BUF_SIZE:\n        buf[ptr:] = data[:BUF_SIZE - ptr]\n        buf[:data_len - (BUF_SIZE - ptr)] = data[BUF_SIZE - ptr:]\n    else:\n        buf[ptr:ptr + len(data)] = data\n    ptr += len(data)\n    ptr %= BUF_SIZE\n    return ptr\n\ndef wrapped_read(read_len, buf, ptr):\n    if ptr + read_len > buf.shape[0]:\n        data = np.concatenate([buf[ptr:], buf[:ptr + read_len - BUF_SIZE]])\n    else:\n        data = buf[ptr:ptr + read_len]\n    ptr += read_len\n    ptr %= BUF_SIZE\n    return data, ptr\n\ndef process_npy(midi, audio):\n    global started\n    global read_stored, write_stored, read_output, write_output\n    _ = wrapped_write(audio[:pyphonic.getBlockSize()], stored_buffer_left, write_stored)\n    write_stored = wrapped_write(audio[pyphonic.getBlockSize():], stored_buffer_right, write_stored)\n\n    started = True\n    left, _ = wrapped_read(2048, stored_buffer_left, read_stored)\n    right, _ = wrapped_read(2048, stored_buffer_right, read_stored)\n    read_stored += pyphonic.getBlockSize()\n    read_stored = read_stored % BUF_SIZE\n\n    f, t, Zxxfl = signal.stft(left, fs=pyphonic.getSampleRate(), nperseg=1024)\n    Zxxfl[TAMPER_START:TAMPER_END] *= np.linspace(0, 1, TAMPER_END - TAMPER_START).reshape(-1, 1)\n    f2, t2, Zxxfr = signal.stft(right, fs=pyphonic.getSampleRate(), nperseg=1024)\n    Zxxfr[TAMPER_START:TAMPER_END] *= np.linspace(0, 1, TAMPER_END - TAMPER_START).reshape(-1, 1)\n\n    _, ifl = signal.istft(Zxxfl, fs=pyphonic.getSampleRate(), nperseg=1024)\n    _, ifr = signal.istft(Zxxfr, fs=pyphonic.getSampleRate(), nperseg=1024)\n\n    _ = wrapped_write(ifl[pyphonic.getBlockSize()*2:pyphonic.getBlockSize()*3], output_buffer_left, write_output)\n    write_output = wrapped_write(ifr[pyphonic.getBlockSize()*2:pyphonic.getBlockSize()*3], output_buffer_right, write_output)\n\n    retval_left, _ = wrapped_read(pyphonic.getBlockSize(), output_buffer_left, read_output)\n    retval_right, read_output = wrapped_read(pyphonic.getBlockSize(), output_buffer_right, read_output)\n\n\n    return midi, np.concatenate([retval_left, retval_right])"
    }
]