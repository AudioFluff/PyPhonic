[
    {
        "id": 1,
        "name": "Init (Plain Python)",
        "code": "# init\n# Do nothing\nimport pyphonic\ndef process(midi, audio):\n    return midi, audio"
    },
    {
        "id": 2,
        "name": "Init (Python with NumPy)",
        "code": "# init (numpy)\n# Do nothing, with NumPy\nimport numpy as np\nimport pyphonic\ndef process_npy(midi, audio):\n    return midi, audio"
    },
    {
        "id": 3,
        "name": "Init (Python with PyTorch)",
        "code": "# init (torch)\n# Do nothing, with PyTorch\nimport torch\nimport pyphonic\ndef process_torch(midi, audio):\n    return midi, audio"
    },
    {
        "id": 4,
        "name": "Butterworth Filter (High, Low or Bandpass)",
        "code": "# butterworth\n# Butterworth filter (High, Low, or Bandpass)\n\nimport pyphonic\n\nimport numpy as np\nfrom scipy.signal import butter, sosfilt\n\norder = 6\nfs = None\nsos = None\ncutoff = 500\nZl, Zr = None, None\n\ndef process_npy(midi, audio):\n    global Zl, Zr, fs, sos\n    if fs is None:\n        fs = pyphonic.getSampleRate()\n        sos = butter(order, cutoff, fs=fs, btype='lowpass', analog=False, output='sos')\n        Zl = np.zeros((sos.shape[0], 2))\n        Zr = np.zeros((sos.shape[0], 2))\n        return midi, audio\n    left = audio[0]\n    right = audio[1]\n    left, Zl = sosfilt(sos, left, zi=Zl)\n    right, Zr = sosfilt(sos, right, zi=Zr)\n    return midi, np.stack((left, right), dtype=np.float32)"
    },
    {
        "id": 5,
        "name": "Midi Arpeggiator, minor triad",
        "code": "# midi_arp\n# Simple MIDI minor triad arpeggiator\n\nimport pyphonic\nfrom pyphonic import MidiMessage\n\ntimer = 0\nmidibuf = {}\ninitial_delay = 0\nduration = 1\narp_delay = 7\nnum_samples = None\n\ndef add_note(beats_from_now, type, note, velocity, channel):\n    global midibuf, timer, num_samples\n\n    bpm = pyphonic.getBPM()\n    \n    bps = bpm / 60\n    ticks_per_second = bps * 1000\n\n    blocks_per_second = pyphonic.getSampleRate() / num_samples\n\n    ticks_per_block = ticks_per_second / blocks_per_second\n\n    tick_skip = 1000 * beats_from_now\n    \n    blocks_in_future = tick_skip / ticks_per_block / 4 # assume 4/4 time\n\n    when = int(timer + blocks_in_future)\n\n    midibuf[when] = midibuf.get(when, [])\n\n    new_note = MidiMessage(type, note, velocity, channel)\n    if type == \"note_off\":\n        new_note.velocity = 0\n    midibuf[when].append(new_note)\n\ndef process(midi, audio):\n    global timer, midibuf, num_samples\n    if num_samples is None:\n        num_samples = len(audio[0])\n    for msg in midi:\n        if msg.type == \"note_on\":\n            when = initial_delay\n            add_note(when, \"note_on\", msg.note, msg.velocity, msg.channel)\n            when += duration\n            add_note(when, \"note_off\", msg.note, msg.velocity, msg.channel)\n            when += arp_delay\n            add_note(when, \"note_on\", msg.note + 3, msg.velocity, msg.channel)\n            when += duration\n            add_note(when, \"note_off\", msg.note + 3, msg.velocity, msg.channel)\n            when += arp_delay\n            add_note(when, \"note_on\", msg.note + 7, msg.velocity, msg.channel)\n            when += duration\n            add_note(when, \"note_off\", msg.note + 7, msg.velocity, msg.channel)\n    \n    for k in list(midibuf.keys()):\n        if k < timer:\n            del midibuf[k]\n\n    timer += 1\n\n    return midibuf.get(timer - 1, []), audio"
    },
    {
        "id": 6,
        "name": "Wavetable Synth",
        "code": "# wavetable_synth\n# First builds wavetables from a sample by pitch shifting, then playable with MIDI.\n\nfrom pathlib import Path\n\nimport pyphonic\nfrom pyphonic import MidiMessage\nimport numpy as np\nimport librosa\n\nsample = np.load(pyphonic.getDataDir() / Path(\"glockenspiel.pkl\"), allow_pickle=True)\n\ndef noteToFreq(midi_note):\n    a = 440\n    return (a / 32) * (2 ** ((midi_note - 9) / 12))\n\nprint(\"Building wavetable\")\nvoices = {}\nfor note in range(31, 103): # G1 to G7\n    print(f\"Building wavetable {note}\")\n    if note == 60:\n        ratio = 1 # Center on C3\n    else:\n        freq = noteToFreq(note)\n        ratio = freq / 261.63\n    left = librosa.effects.pitch_shift(sample[0], sr=44100, n_steps=(note - 60))\n    right = librosa.effects.pitch_shift(sample[1], sr=44100, n_steps=(note - 60))\n    joined = np.array([left, right])\n    voices[note] = {\"wave\": joined, \"position\": 0, \"playing\": False, \"velocity\": 0}\n\ndef process_npy(midi, audio):\n\n    num_channels, num_samples = audio.shape\n\n    for msg in midi:\n        if msg.note not in voices:\n            continue\n        if msg.type == \"note_on\":\n            if voices[msg.note][\"playing\"]:\n                voices[msg.note][\"position\"] = 0\n            else:\n                voices[msg.note][\"playing\"] = True\n            voices[msg.note][\"velocity\"] = msg.velocity\n        elif msg.type == \"note_off\":\n            voices[msg.note][\"position\"] = 0\n            voices[msg.note][\"playing\"] = False\n            # Could add some tail off instead of dead stop\n    \n    new_audio = np.zeros((num_channels, num_samples), dtype=np.float32)\n\n    for voice, data in voices.items():\n        if not data[\"playing\"]:\n            continue\n        start_pos = data[\"position\"] % data[\"wave\"].shape[1]\n        end_pos = (start_pos + num_samples)\n        if end_pos >= data[\"wave\"].shape[1]:\n            end_pos = data[\"wave\"].shape[1]\n            new_audio[:, :end_pos - start_pos] += data[\"wave\"][:, start_pos:end_pos]\n            voices[voice][\"position\"] = 0\n            voices[voice][\"playing\"] = False\n        else:\n            new_audio += data[\"wave\"][:, start_pos:end_pos] * (data[\"velocity\"] / 127)\n            voices[voice][\"position\"] += num_samples\n    \n    return midi, new_audio"
    },
    {
        "id": 7,
        "name": "Time Stretching Sampler",
        "code": "# time_stretching_sampler\n# Stretches the duration of a sample without affecting pitch. Playable with MIDI; samples will start looping faster as they get shorter.\n\nfrom pathlib import Path\n\nimport pyphonic\nfrom pyphonic import MidiMessage\nimport numpy as np\nimport librosa\n\nsample = np.load(pyphonic.getDataDir() / Path(\"glockenspiel.pkl\"), allow_pickle=True)\n\ndef noteToFreq(midi_note):\n    a = 440\n    return (a / 32) * (2 ** ((midi_note - 9) / 12))\n\nprint(\"Building wavetable\")\nvoices = {}\nfor note in range(31, 103): # G1 to G7\n    print(f\"Building wavetable {note}\")\n    if note == 60:\n        ratio = 1 # Center on C3\n    else:\n        freq = noteToFreq(note)\n        ratio = freq / 261.63\n    left = librosa.effects.time_stretch(sample[0], rate=ratio)\n    right = librosa.effects.time_stretch(sample[1], rate=ratio)\n    joined = np.array([left, right])\n    voices[note] = {\"wave\": joined, \"position\": 0, \"playing\": False, \"velocity\": 0}\n\ndef process_npy(midi, audio):\n\n    num_channels, num_samples = audio.shape\n\n    for msg in midi:\n        if msg.note not in voices:\n            continue\n        if msg.type == \"note_on\":\n            if voices[msg.note][\"playing\"]:\n                voices[msg.note][\"position\"] = 0\n            else:\n                voices[msg.note][\"playing\"] = True\n            voices[msg.note][\"velocity\"] = msg.velocity\n        elif msg.type == \"note_off\":\n            voices[msg.note][\"position\"] = 0\n            voices[msg.note][\"playing\"] = False\n    \n    new_audio = np.zeros((num_samples, num_channels), dtype=np.float32)\n\n    for voice, data in voices.items():\n        if not data[\"playing\"]:\n            continue\n        start_pos = data[\"position\"] % data[\"wave\"].shape[1]\n        end_pos = (start_pos + num_samples)\n        if end_pos >= data[\"wave\"].shape[1]:\n            end_pos = data[\"wave\"].shape[1]\n            new_audio[:, :end_pos - start_pos] += data[\"wave\"][:, start_pos:end_pos]\n            voices[voice][\"position\"] = 0\n        else:\n            new_audio += data[\"wave\"][:, start_pos:end_pos] * (data[\"velocity\"] / 127)\n            voices[voice][\"position\"] += num_samples\n    \n    return midi, new_audio"
    },
    {
        "id": 8,
        "name": "Noise (PyTorch)",
        "code": "# Noise\n# Just adds some configurable noise to the input audio\nimport torch\n\nimport pyphonic\n\nNOISE_VOLUME = 0.1\n# 0.01 is a vinyl crackle, 0.1 is a light rain, 0.5 is a heavy rain, 1 is a thunderstorm\nNOISE_DENSITY = NOISE_VOLUME * 0.5\n\ndef process_torch(midi, audio):\n    noise = torch.randn_like(audio) * NOISE_VOLUME\n    noise = noise * (torch.rand_like(noise).abs() < NOISE_DENSITY)\n    return midi, audio + noise"
    },
    {
        "id": 9,
        "name": "Saturator (PyTorch)",
        "code": "# Saturator\n# Use pitch wheel to control saturation level\n\nimport torch\n\nimport pyphonic\n\n# This adds oomph to EDM drums\n# DEW_POINT = 0.9\n# SHIFT = 0\n# FOLDBACK_POINT = 0.8\n# COMP_GAIN = 1.1\n\n# This is a Brit guitar amp\nDEW_POINT = 0.1\nSHIFT = 0.005\nFOLDBACK_POINT = 0.8\nCOMP_GAIN = 3.0\n\nmodified_dew_point = DEW_POINT\n\ndef process_torch(midi, audio):\n    global modified_dew_point\n    for cc in midi:\n        if cc.type == \"pitch_wheel_change\":\n            modified_dew_point = DEW_POINT * ((cc.velocity + 127) / 127)\n            print(modified_dew_point)\n\n    audio[audio < DEW_POINT * audio.min()] = DEW_POINT * audio.min() + SHIFT\n    audio[audio > DEW_POINT * audio.max()] = DEW_POINT * audio.max() + SHIFT\n    audio[audio > FOLDBACK_POINT] = FOLDBACK_POINT - audio[audio > FOLDBACK_POINT]\n    audio[audio < -FOLDBACK_POINT] = -FOLDBACK_POINT + -audio[audio < -FOLDBACK_POINT]\n    return midi, audio * COMP_GAIN"
    },
    {
        "id": 10,
        "name": "Ramping FFT-based filter",
        "code": "# fft\n# Ramped FFT-based filtering over the FFT bins specified as TAMPER_START and TAMPER_END\n\nimport numpy as np\nfrom scipy import signal\n\nimport pyphonic\n\nBUF_SIZE = 8820\n\nstored_buffer_left = np.zeros((BUF_SIZE, ), dtype=np.float32)\nstored_buffer_right = np.zeros((BUF_SIZE, ), dtype=np.float32)\noutput_buffer_left = np.zeros((BUF_SIZE, ), dtype=np.float32)\noutput_buffer_right = np.zeros((BUF_SIZE, ), dtype=np.float32)\n\nread_stored, write_stored = 0, 0\nread_output, write_output = 0, 0\nstarted = False\n\nTAMPER_START, TAMPER_END = 10, 40\n\ndef wrapped_write(data, buf, ptr):\n    data_len = data.shape[0]\n    if ptr + data_len > BUF_SIZE:\n        buf[ptr:] = data[:BUF_SIZE - ptr]\n        buf[:data_len - (BUF_SIZE - ptr)] = data[BUF_SIZE - ptr:]\n    else:\n        buf[ptr:ptr + len(data)] = data\n    ptr += len(data)\n    ptr %= BUF_SIZE\n    return ptr\n\ndef wrapped_read(read_len, buf, ptr):\n    if ptr + read_len > buf.shape[0]:\n        data = np.concatenate([buf[ptr:], buf[:ptr + read_len - BUF_SIZE]])\n    else:\n        data = buf[ptr:ptr + read_len]\n    ptr += read_len\n    ptr %= BUF_SIZE\n    return data, ptr\n\ndef process_npy(midi, audio):\n    global started\n    global read_stored, write_stored, read_output, write_output\n\n    num_channels, num_samples = audio.shape\n\n    _ = wrapped_write(audio[0], stored_buffer_left, write_stored)\n    write_stored = wrapped_write(audio[1], stored_buffer_right, write_stored)\n\n    started = True\n    left, _ = wrapped_read(2048, stored_buffer_left, read_stored)\n    right, _ = wrapped_read(2048, stored_buffer_right, read_stored)\n    read_stored += num_samples\n    read_stored = read_stored % BUF_SIZE\n\n    f, t, Zxxfl = signal.stft(left, fs=pyphonic.getSampleRate(), nperseg=1024)\n    Zxxfl[TAMPER_START:TAMPER_END] *= np.linspace(0, 1, TAMPER_END - TAMPER_START).reshape(-1, 1)\n    f2, t2, Zxxfr = signal.stft(right, fs=pyphonic.getSampleRate(), nperseg=1024)\n    Zxxfr[TAMPER_START:TAMPER_END] *= np.linspace(0, 1, TAMPER_END - TAMPER_START).reshape(-1, 1)\n\n    _, ifl = signal.istft(Zxxfl, fs=pyphonic.getSampleRate(), nperseg=1024)\n    _, ifr = signal.istft(Zxxfr, fs=pyphonic.getSampleRate(), nperseg=1024)\n\n    _ = wrapped_write(ifl[num_samples*2:num_samples*3], output_buffer_left, write_output)\n    write_output = wrapped_write(ifr[num_samples*2:num_samples*3], output_buffer_right, write_output)\n\n    retval_left, _ = wrapped_read(num_samples, output_buffer_left, read_output)\n    retval_right, read_output = wrapped_read(num_samples, output_buffer_right, read_output)\n\n\n    return midi, np.stack([retval_left, retval_right])"
    },
    {
        "id": 11,
        "name": "Poly Sine Synth",
        "code": "# polysine\n# A simple polyphonic sine wave synthesizer\n\nimport math\nimport pyphonic\n\nclass Synth:\n    def __init__(self, sample_rate=44100):\n        self.angleDelta = 0.0\n        self.currentAngle = 0.0\n        self.tail = None\n        self.stopped = True\n        self.sample_rate = sample_rate # default, will be set by vst\n\n    def start_note(self, note, vel):\n        self.currentAngle = 0.0\n        self.level = vel * 0.05\n        self.tail = None\n        self.stopped = False\n\n        def noteToFreq(midi_note):\n            a = 440\n            return (a / 32) * (2 ** ((midi_note - 9) / 12))\n        self.cyclesPerSecond = noteToFreq(note)\n        self.cyclesPerSample = self.cyclesPerSecond / self.sample_rate\n        self.angleDelta = self.cyclesPerSample * 2 * math.pi\n    \n    def sin(self, val):\n        return math.sin(val)\n    \n    def stop_note(self):\n        self.tail = 50000\n    \n    def renew(self):\n        self.tail = None\n    \n    def is_active(self):\n        if self.stopped:\n            return False\n        return True\n    \n    def render(self, num):\n        is_stopping = self.tail is not None\n        buf = []\n        if abs(self.angleDelta) != 0 and not self.stopped:\n            while num > 0:\n                num -= 1\n                cur = self.sin(self.currentAngle) * self.level\n                if self.tail is not None:\n                    cur *= self.tail / 50000\n                    self.tail = self.tail - 1\n                buf.append(cur)\n                self.currentAngle += self.angleDelta\n        else:\n            buf = [0.0 for _ in range(num)]\n        \n        if is_stopping and self.tail <= 0:\n            self.tail = None\n            self.angleDelta = 0.0\n            self.stopped = True\n\n        return buf\n\nclass Poly:\n\n    def __init__(self, sample_rate=44100, block_size=64):\n        self.synths = {}\n        self.delay_buf = [0.0] * 12000\n        self.delay_position = 0\n        self.sample_rate = sample_rate\n        self.block_size = block_size\n\n    def set_sample_rate_block_size(self, rate, block_size):\n        self.sample_rate = rate\n        self.block_size = block_size\n\n    def start_note(self, note, vel):\n        if note not in self.synths:\n            self.synths[note] = Synth(sample_rate=self.sample_rate)\n            self.synths[note].start_note(note, vel)\n        elif not self.synths[note].is_active():\n            self.synths[note].start_note(note, vel)\n        else:\n            self.synths[note].renew()\n    \n    def stop_note(self, note):\n        if note in self.synths:\n            self.synths[note].stop_note()\n    \n    def render(self):\n\n        if not len(self.synths):\n            cur = [0.0] * self.block_size\n        else:\n            bufs = []\n            one_active = False\n            for note, synth in self.synths.items():\n                if synth.is_active():\n                    one_active = True\n                    bufs.append(synth.render(self.block_size))\n            if not one_active:\n                cur = [0.0] * self.block_size\n            else:\n                cur = [sum(x) for x in zip(*bufs)]\n\n        for i in range(0, self.block_size):\n            in_ = cur[i]\n            cur[i] += self.delay_buf[self.delay_position]\n            self.delay_buf[self.delay_position] = (self.delay_buf[self.delay_position] + in_) * 0.5\n            self.delay_position += 1\n            if self.delay_position >= len(self.delay_buf):\n                self.delay_position = 0\n\n        return cur\n\npoly = Poly()\n\ndef process(midi_messages, audio):\n    num_samples = len(audio[0])\n    poly.set_sample_rate_block_size(pyphonic.getSampleRate(), num_samples)\n    for m in midi_messages:\n        if m.type == \"note_on\":\n            if m.note < 20:\n                continue\n            poly.start_note(m.note, m.velocity/10)\n        elif m.type == \"note_off\":\n            poly.stop_note(m.note)\n        else:\n            print(m)\n    \n    render = poly.render()\n    return midi_messages, [render, render] # stereo\n"
    },
    {
        "id": 12,
        "name": "Beat Synced Noise",
        "code": "# beat_synced_noise\n# Add some fizz to your chorus with noise. 1000 ticks per beat, so starting at 250 is a 16th after each beat.\n\nimport numpy as np\n\nimport pyphonic\n\nNOISE_VOLUME = 0.2\nNOISE_DENSITY = NOISE_VOLUME * 0.5\nstart_ticks, end_ticks = 250, 300\n\ndef process_npy(midi, audio):\n    if start_ticks <= pyphonic.getTransport()[\"ticks\"] < end_ticks:\n        noise = np.random.normal(scale=NOISE_VOLUME, size=audio.shape)\n        noise = (np.abs(np.random.normal(size=audio.shape)) < NOISE_DENSITY) * noise\n        return midi, audio + noise.astype(np.float32)\n    return midi, audio"
    },
    {
        "id": 13,
        "name": "Sidechain Compressor (Ducking)",
        "code": "# sc_compressor\n# Fake sidechain compressor to duck audio on the kick\n\nimport numpy as np\n\nimport pyphonic\n\nATTENUATION = 0.5\nstart_ticks, end_ticks = 0, 250\n\ndef process_npy(midi, audio):\n    if start_ticks <= pyphonic.getTransport()[\"ticks\"] < end_ticks:\n        return midi, audio * ATTENUATION\n    return midi, audio"
    },
    {
        "id": 14,
        "name": "NN Voice Changer (PyTorch, CUDA)",
        "code": "# speecht5_voice_change\n# SpeechT5 model converts voice to another speaker.\n# This will take a long time to initialize at first run (downloading weights).\n# Output will start when input has been quiet for WAIT_FOR_QUIET blocks.\n# As is, *THIS NEEDS CUDA*. You can modify it to run on CPU.\n# Consider increasing your DAW's latency/buffer size for better results.\n\nimport threading\nfrom pathlib import Path\n\nimport librosa\nimport pyphonic\nimport numpy as np\nimport torch\n\nfrom transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\n\nMALE = \"cmu_us_bdl_arctic-wav-arctic_a0009.npy\"\nFEMALE = \"cmu_us_slt_arctic-wav-arctic_a0508.npy\"\nWAIT_FOR_QUIET = 20\n\ncheckpoint = \"microsoft/speecht5_vc\"\npreprocessor = SpeechT5Processor.from_pretrained(checkpoint)\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained(checkpoint).cuda()\nvocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").cuda()\n\nblock_num = 0\nblocks = [[]]\noutput_block_num = 0\noutput_block_ptr = 0\noutput_blocks = [[]]\nquiet_for = 0\nevent = threading.Event()\nready_to_trigger = True\n\nspeaker_embedding = np.load(Path(pyphonic.getDataDir()) / FEMALE)\nspeaker_embedding = torch.tensor(speaker_embedding, requires_grad=False).unsqueeze(0).cuda()\n\ndef process_audio(sampling_rate, waveform):\n    if len(waveform.shape) > 1:\n        waveform = librosa.to_mono(waveform.T)\n    if sampling_rate != 16000:\n        waveform = librosa.resample(waveform, orig_sr=sampling_rate, target_sr=16000)\n    waveform = torch.tensor(waveform, requires_grad=False)\n    return waveform\n\ndef predict(audio):\n    waveform = process_audio(44100, audio)\n    inputs = preprocessor(audio=waveform, sampling_rate=16000, return_tensors=\"pt\")\n    if inputs[\"input_values\"].shape[1] < 1024:\n        return\n    inputs = inputs[\"input_values\"].contiguous().cuda()\n    print(inputs.device)\n    with torch.no_grad():\n        speech = model.generate_speech(inputs, speaker_embedding, vocoder=vocoder)\n    speech = speech.contiguous().cpu().numpy().astype(np.float32)\n    return speech\n\ndef processor_thread():\n    global block_num, blocks, event\n    global output_block_num\n    while True:\n        event.wait()\n        if not len(blocks[block_num]):\n            continue\n        \n        signal_to_do_processing_on = np.concatenate(blocks[block_num])\n        \n        wave = predict(signal_to_do_processing_on)\n        if wave is None:\n            continue\n        wave = librosa.resample(wave, orig_sr=16000, target_sr=44100)\n\n        output_blocks[output_block_num] = wave\n        block_num += 1\n        blocks.append([])\n        event.clear()\n        print(block_num)\n        blocks[block_num - 1] = None\n\nprocessor = threading.Thread(target=processor_thread)\nprocessor.daemon = True\nprocessor.start()\n\ndef process_npy(midi, audio):\n    global quiet_for, output_block_num, output_block_ptr, output_blocks\n    global event, block_num, ready_to_trigger\n\n    num_channels, num_samples = audio.shape\n\n    if num_channels == 2:\n        ready_audio = (audio[0] + audio[1]) / 2.0\n    else:\n        ready_audio = audio[0]\n    \n    if pyphonic.getSignalStats()[\"max\"] < 0.01:\n        quiet_for += 1\n        if quiet_for > WAIT_FOR_QUIET and ready_to_trigger:\n            quiet_for = 0\n            event.set()\n    else:\n        blocks[block_num].append(ready_audio)\n        quiet_for = 0\n    \n    if len(output_blocks[output_block_num]):\n        if output_block_ptr + num_samples >= len(output_blocks[output_block_num]):\n            output_block_num += 1\n            output_blocks.append([])\n            output_block_ptr = 0\n            retval = output_blocks[output_block_num][output_block_ptr:]\n            pad = num_samples - len(retval)\n            retval = np.concatenate([retval, np.zeros([pad,])], dtype=np.float32)\n            output_blocks[output_block_num - 1] = None\n            ready_to_trigger = True\n        else:\n            retval = output_blocks[output_block_num][output_block_ptr:output_block_ptr+num_samples]\n            output_block_ptr += num_samples\n            ready_to_trigger = False\n\n        if num_channels == 2:\n            return midi, np.stack([retval, retval])\n        else:\n            return midi, np.expand_dims(retval, 0)\n\n    return midi, np.zeros_like(audio)\n\n# Credits:\n# Matthijs Hollemans https://huggingface.co/spaces/Matthijs/speecht5-vc-demo\n# Ao, Wang et al https://arxiv.org/abs/2110.07205, https://huggingface.co/mechanicalsea/speecht5-vc\n# Microsoft https://github.com/microsoft/SpeechT5\n# CMU Festvox http://www.festvox.org/cmu_arctic/"
    },
    {
        "id": 15,
        "name": "Midi Chord Generator",
        "code": "# midi_gen_markovian\n# Generates MIDI chords. Pipe it into a lush pad, *HIT PLAY*, survive the ambient and wait for the epic trance progression.\n\nimport random\n\nimport pyphonic\nfrom pyphonic import MidiMessage\n\nlast_bar = -1\nfirst_of_bar = True\nlast_chord = None\nroot = 60 # C4\nchords = {\n  \"I\": { \n    \"notes\": [root, root + 4, root + 7],\n    \"next\": [\"IV\", \"V\"]\n  },\n  \"V\": {\n    \"notes\": [root - 5, root - 1, root + 2],\n    \"next\": [\"vi\"]\n  },\n  \"vi\": {\n    \"notes\": [root - 3, root, root + 4, root + 9],\n    \"next\": [\"IV\", \"V\", \"viinv\", \"IVoct\"]\n  },\n  \"viinv\": {\n    \"notes\": [root - 3, root, root + 4, root + 9],\n    \"next\": [\"IV\", \"V\", \"I\"]\n  },\n  \"IV\": {\n    \"notes\": [root - 7, root - 3, root],\n    \"next\": [\"I\", \"vi\", \"V\"]\n  },\n  \"IVoct\": {\n    \"notes\": [root - 7, root - 3, root, root + 5],\n    \"next\": [\"I\", \"vi\"]\n  }\n}\n\ndef gen_new_chord(last_chord):\n    if last_chord is None:\n        return \"I\"\n    next_ = random.choice(chords[last_chord][\"next\"])\n    return next_\n\ndef process(midi, audio):\n    global last_chord, last_bar, first_of_bar\n    current_bar = pyphonic.getTransport()[\"bar\"] % 8\n\n    if last_bar != current_bar:\n        first_of_bar = True\n        last_bar = current_bar\n    else:\n        first_of_bar = False\n    \n    if first_of_bar and current_bar in (0, 2, 4, 5, 6):\n        if current_bar == 5:\n            if random.random() < 0.7:\n                return midi, audio\n\n        if last_chord:\n            for note in chords[last_chord][\"notes\"]:\n                midi.append(MidiMessage(\"note_off\", note, 127, 0))\n        last_chord = gen_new_chord(last_chord)\n        for note in chords[last_chord][\"notes\"]:\n            midi.append(MidiMessage(\"note_on\", note, 127, 0))\n\n    return midi, audio"
    },
    {
        "id": 16,
        "name": "MIDI Generative Drums",
        "code": "# Generative drum machine\n# Outputs MIDI (C3-G3) to pipe to a sampler. Hold one or all of those notes down, press play.\n\nimport random\n\nimport pyphonic\nfrom pyphonic import MidiMessage\n\nmidi_notes = {x: False for x in range(128)}\ncurrent_pattern = {}\ngenerated_patterns = {}\nstart = None\nlast_bar = -1\nlast_quarter = -1\n\ninstruments = {\n    \"kick\": 60,\n    \"snare\": 62,\n    \"hat\": 64,\n    \"openhat\": 65,\n    \"perc\": 67\n}\nkick_patterns = [\n    [(1, 2, 127), (5, 6, 127), (9, 10, 127), (13, 14, 127)],\n    [(1, 4, 127), (8, 9, 127), (9, 13, 127)],\n    [(1, 4, 127), (8, 9, 127), (9, 10, 127), (11, 11, 45)],\n    [(1, 4, 127), (8, 9, 35), (9, 10, 127), (11, 11, 45)],\n    [(1, 2, 127), (9, 10, 127)],\n    [(1, 2, 127), (6, 6, 127), (9, 10, 127), (14, 14, 127)],\n    [(1, 2, 127), (5, 6, 127), (9, 10, 127), (12, 12, 30), (13, 14, 110)],\n]\nsnare_patterns = [\n    [(5, 6, 127), (13, 15, 127)],\n    [(5, 6, 127), (13, 15, 127), (16, 16, 127)],\n    [(5, 6, 127), (13, 15, 127), (16, 16, 127)],\n    [(5, 6, 127), (10, 10, 63), (13, 15, 127), (16, 16, 127)],\n    [(2, 3, 127), (13, 14, 127)],\n]\nclosed_hat_patterns = [\n    [(1, 1, 127), (2, 2, 127), (3, 3, 127), (4, 4, 127), (5, 5, 127), (6, 6, 127), (7, 7, 127), (8, 8, 127), (9, 9, 127), (10, 10, 127), (11, 11, 127), (12, 12, 127), (13, 13, 127), (14, 14, 127), (15, 15, 127), (16, 16, 127)],\n    [(3, 3, 127), (4, 4, 127), (7, 7, 127), (8, 8, 127), (11, 11, 127), (12, 12, 127), (15, 15, 127), (16, 16, 127)],\n]\nopen_hat_patterns = [\n    [(3, 3, 80), (7, 7, 80), (11, 11, 80), (15, 15, 80)],\n    [(1, 2, 30), (5, 6, 30), (9, 10, 30), (13, 14, 60)],\n]\n\n\ndef is_note_on(note):\n    return midi_notes[note]\ndef process_midi(midi):\n    global midi_notes\n    for msg in midi:\n        if msg.type == \"note_on\":\n            midi_notes[msg.note] = True\n        elif msg.type == \"note_off\":\n            midi_notes[msg.note] = False\n        elif msg.type == \"channel_pressure\":\n            print(f\"New pattern {msg.note}\")\n            generate_pattern(msg.note)\n    return None\ndef generate_perc():\n    i = 0\n    perc = []\n    while i < 16:\n        if random.random() < 0.4:\n            next_ = random.randint(0, 4)\n            perc.append((i, i + next_, random.randint(1, 127)))\n        else:\n            next_ = 1\n        i += next_\n    return perc\ndef generate_pattern(num):\n    global current_pattern\n    if generated_patterns.get(num):\n        current_pattern = generated_patterns[num]\n    else:\n        current_pattern = {\n            \"kick\": random.choice(kick_patterns),\n            \"snare\": random.choice(snare_patterns),\n            \"hat\": random.choice(closed_hat_patterns),\n            \"openhat\": random.choice(open_hat_patterns),\n            \"perc\": generate_perc(),\n        }\n        generated_patterns[num] = current_pattern\n\n    return current_pattern\ndef play_pattern(cur_16th):\n    global current_pattern\n    midi_out = []\n    for hit in current_pattern:\n        for start, stop, vel in current_pattern[hit]:\n            if cur_16th == start and is_note_on(instruments[hit]):\n                midi_out.append(MidiMessage(\"note_on\", instruments[hit], vel, 0))\n            if cur_16th == stop:\n                midi_out.append(MidiMessage(\"note_on\", instruments[hit], 0, 0))\n    return midi_out\n\ngenerate_pattern(0)\n\ndef get_16th():\n    global start, last_bar, last_quarter\n    transport = pyphonic.getTransport()\n    if start is None:\n        start = transport[\"sample_num\"]\n    elapsed = transport[\"sample_num\"] - start\n    bpm = pyphonic.getBPM()\n    samples_per_beat = 60 * 44100 / bpm\n    quarter = int((elapsed / 44100 * 8) % samples_per_beat)\n    if transport[\"bar\"] != last_bar:\n        start = transport[\"sample_num\"]\n        last_bar = transport[\"bar\"]\n    if quarter != last_quarter:\n        num_16 = quarter + 1\n    else:\n        num_16 = None\n    last_quarter = quarter\n    return num_16\n\n\ndef process(midi, audio):\n    global start, last_bar, last_quarter\n    \n    process_midi(midi)\n\n    num_16 = get_16th()\n    \n    midi_out = play_pattern(num_16)\n\n    return midi_out, audio"
    }
]